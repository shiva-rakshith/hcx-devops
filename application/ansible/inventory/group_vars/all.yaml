env: dev
docker_repo: swasth2021
keycloak_postgresql_host: ""
keycloak_postgresql_user: ""
# Ref: https://www.middlewareinventory.com/blog/ansible-get-ip-address/
elasticsearch_network_host: "hostvars[inventory_hostname]['ansible_env'].SSH_CONNECTION.split(' ')[2]"

hcx_api_postgresql_host: "{{ keycloak_postgresql_host }}"
hcx_api_postgresql_user: "{{ keycloak_postgresql_user }}"
hcx_api_postgresql_password: "{{ keycloak_postgresql_password }}"
hcx_registry_postgresql_user: "{{ keycloak_postgresql_user }}"
hcx_registry_postgresql_password: "{{ keycloak_postgresql_password }}"
hcx_registry_postgresql_host: "{{ keycloak_postgresql_host }}"

kafka_topics:
  - "{{ env }}.hcx.request.claim"
  - "{{ env }}.hcx.request.coverageeligibility"
  - "{{ env }}.hcx.request.ingest"
  - "{{ env }}.hcx.request.payload"
  - "{{ env }}.hcx.request.payment"
  - "{{ env }}.hcx.request.preauth"
  - "{{ env }}.hcx.request.retry"
  - "{{ env }}.hcx.request.search"
  - "{{ env }}.hcx.request.status.search"
  - "{{ env }}.hcx.response.search"
  - "{{ env }}.hcx.request.communication"
  - "{{ env }}.hcx.request.predetermination"
  - "{{ env }}.hcx.request.notification"
  - "{{ env }}.hcx.request.subscription"
  - "{{ env }}.hcx.audit"

flink_job_configs:
  base_config: |
    kafka {
      broker-servers = "{{ groups['kafka'] | join(':9092,') }}:9092"
      zookeeper = "{{ groups['zookeeper'] | join(':2181,') }}:2181"
        producer {
          max-request-size = 1572864
        }
      }
      job {
        env = "{{ env }}"
        enable.distributed.checkpointing = false
        statebackend {
          blob {
            storage {
              account = "local.blob.core.windows.net"
              container = "local"
              checkpointing.dir = "checkpoint"
            }
          }
          base.url = "wasbs://"${job.statebackend.blob.storage.container}"@"${job.statebackend.blob.storage.account}"/"${job.statebackend.blob.storage.checkpointing.dir}
        }
      }
      task {
        parallelism = 1
        consumer.parallelism = 1
        checkpointing.compressed = true
        checkpointing.interval = 60000
        checkpointing.pause.between.seconds = 30000
        restart-strategy.attempts = 3
        restart-strategy.delay = 30000
      }
      postgres {
        host = "{{ flink_postgresql_host | default(keycloak_postgresql_host) }}"
        port = 5432
        maxConnections = 2
        user = "{{ flink_postgresql_user | default(keycloak_postgresql_user) }}"
        password = "{{ flink_postgresql_password | default(keycloak_postgresql_password) }}"
        database: postgres
        table: payload
      }
      redis {
        host = redis-master.{{env}}.svc.cluster.local
        port = 6379
        expires = 3600
      }
      redisdb{
        connection {
          timeout: 30000
          }
        assetstore {
          id = 0
          }
        }
      redis-meta {
        host = redis-master.{{env}}.svc.cluster.local
        port = 6379
      }
      lms-cassandra {
        host = "localhost"
        port = "9042"
      }
      es {
        basePath = "{{ groups['elasticsearch'][0] }}:9200"
        batchSize = 1000
      }
      registry {
        hcx.code = {{ registry_hcx_code }}
      }
      hcx-apis{
        endPointUrl = "http://hcx-api.{{ env }}.svc.cluster.local:8080"
      }
      max.retry = 3
      allowedEntitiesForRetry = ["coverageeligibility", "preauth", "claim"]
      audit {
        index = "hcx_audit"
        alias = "hcx_audit"
        timezone = "IST"
      }
      jwt-token {
        privateKey = "MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCG+XLPYiCxrZq71IX+w7uoDGxGI7qy7XaDbL3BJE33ju7rjdrP7wsAOWRvM8BIyWuRZZhl9xG+u7l/7OsZAzGoqI7p+32x+r9IJVzboLDajk6tp/NPg1csc7f2M5Bu6rkLEvrKLz3dgy3Q928rMsD3rSmzBLelfKTo+aDXvCOiw1dMWsZZdkEpCTJxH39Nb2K4S59kO/R2GtSU/QMLq65m34XcMZpDtatA1u1S8JdZNNeMCO+NuFKBzIfvXUCQ8jkf7h612+UP1AYhoyCMFpzUZ9b7liQF9TYpX1Myr/tT75WKuRlkFlcALUrtVskL8KA0w6sA0nX5fORVsuVehVeDAgMBAAECggEAX1n1y5/M7PhxqWO3zYTFGzC7hMlU6XZsFOhLHRjio5KsImgyPlbm9J+W3iA3JLR2c17MTKxAMvg3UbIzW5YwDLAXViC+aW90like8mEQzzVdS7ysXG2ytcqCGUHQNStI0hP0a8T39XbodQl31ZKjU9VW8grRGe12Kse+4ukcW6yRVES+CkyO5BQB+vs3voZavodRGsk/YSt00PtIrFPJgkDuyzzcybKJD9zeJk5W3OGVK1z0on+NXKekRti5FBx/uEkT3+knkz7ZlTDNcyexyeiv7zSL/L6tcszV0Fe0g9vJktqnenEyh4BgbqABPzQR++DaCgW5zsFiQuD0hMadoQKBgQC+rekgpBHsPnbjQ2Ptog9cFzGY6LRGXxVcY7hKBtAZOKAKus5RmMi7Uv7aYJgtX2jt6QJMuE90JLEgdO2vxYG5V7H6Tx+HqH7ftCGZq70A9jFBaba04QAp0r4TnD6v/LM+PGVT8FKtggp+o7gZqXYlSVFm6YzI37G08w43t2j2aQKBgQC1Nluxop8w6pmHxabaFXYomNckziBNMML5GjXW6b0xrzlnZo0p0lTuDtUy2xjaRWRYxb/1lu//LIrWqSGtzu+1mdmV2RbOd26PArKw0pYpXhKFu/W7r6n64/iCisoMJGWSRJVK9X3D4AjPaWOtE+jUTBLOk0lqPJP8K6yiCA6ZCwKBgDLtgDaXm7HdfSN1/Fqbzj5qc3TDsmKZQrtKZw5eg3Y5CYXUHwbsJ7DgmfD5m6uCsCPa+CJFl/MNWcGxeUpZFizKn16bg3BYMIrPMao5lGGNX9p4wbPN5J1HDD1wnc2jULxupSGmLm7pLKRmVeWEvWl4C6XQ+ykrlesef82hzwcBAoGBAKGY3v4y4jlSDCXaqadzWhJr8ffdZUrQwB46NGb5vADxnIRMHHh+G8TLL26RmcET/p93gW518oGg7BLvcpw3nOZaU4HgvQjT0qDvrAApW0V6oZPnAQUlarTU1Uk8kV9wma9tP6E/+K5TPCgSeJPg3FFtoZvcFq0JZoKLRACepL3vAoGAMAUHmNHvDI+v0eyQjQxlmeAscuW0KVAQQR3OdwEwTwdFhp9Il7/mslN1DLBddhj6WtVKLXu85RIGY8I2NhMXLFMgl+q+mvKMFmcTLSJb5bJHyMz/foenGA/3Yl50h9dJRFItApGuEJo/30cG+VmYo2rjtEifktX4mDfbgLsNwsI="
        expiryTime = 300000
      }
      errorCodes {
        successCodes = [200, 202]
        errorCodes = [400, 401, 403, 404]
      }
      hcx {
        instanceName = "swasth-hcx-dev"
      }

  coverageeligibility:
    coverageeligibility: |+
      include file("/data/flink/conf/base-config.conf")
      kafka.groupId:  ${job.env}"-coverage-eligibility-check-group"
      kafka.input.topic: {{ env }}.hcx.request.coverageeligibility
      kafka.audit.topic: {{ env }}.hcx.audit
      task.consumer.parallelism :  1
      task.parallelism :  1
      task.downstream.operators.parallelism: 1
      kafka.producer.max-request-size: 1572864
      kafka.producer.batch.size: 98304
      kafka.producer.linger.ms: 10
    flink-conf: |+
      jobmanager.memory.flink.size: 1024m
      taskmanager.memory.flink.size: 1024m
      taskmanager.numberOfTaskSlots: 1
      parallelism.default: 1
      jobmanager.execution.failover-strategy: region
      taskmanager.memory.network.fraction: 0.1

  preauthjob:
    preauthjob: |+
      include file("/data/flink/conf/base-config.conf")
      kafka.groupId:  ${job.env}"-preauth-job-group"
      kafka.input.topic: {{ env }}.hcx.request.preauth
      kafka.audit.topic: {{ env }}.hcx.audit
      task.consumer.parallelism :  1
      task.parallelism :  1
      task.downstream.operators.parallelism: 1
      kafka.producer.max-request-size: 1572864
      kafka.producer.batch.size: 98304
      kafka.producer.linger.ms: 10
    flink-conf: |+
      jobmanager.memory.flink.size: 1024m
      taskmanager.memory.flink.size: 1024m
      taskmanager.numberOfTaskSlots: 1
      parallelism.default: 1
      jobmanager.execution.failover-strategy: region
      taskmanager.memory.network.fraction: 0.1

  claimsjob:
    claimsjob: |+
      include file("/data/flink/conf/base-config.conf")
      kafka.groupId:  ${job.env}"-claims-job-group"
      kafka.input.topic: {{ env }}.hcx.request.claim
      kafka.audit.topic: {{ env }}.hcx.audit
      task.consumer.parallelism :  1
      task.parallelism :  1
      task.downstream.operators.parallelism: 1
      kafka.producer.max-request-size: 1572864
      kafka.producer.batch.size: 98304
      kafka.producer.linger.ms: 10

    flink-conf: |+
      jobmanager.memory.flink.size: 1024m
      taskmanager.memory.flink.size: 1024m
      taskmanager.numberOfTaskSlots: 1
      parallelism.default: 1
      jobmanager.execution.failover-strategy: region
      taskmanager.memory.network.fraction: 0.1

  paymentsjob:
    paymentsjob: |+
      include file("/data/flink/conf/base-config.conf")
      kafka.groupId:  ${job.env}"-claims-job-group"
      kafka.input.topic: {{ env }}.hcx.request.payment
      kafka.audit.topic: {{ env }}.hcx.audit
      task.consumer.parallelism :  1
      task.parallelism :  1
      task.downstream.operators.parallelism: 1
      kafka.producer.max-request-size: 1572864
      kafka.producer.batch.size: 98304
      kafka.producer.linger.ms: 10

    flink-conf: |+
      jobmanager.memory.flink.size: 1024m
      taskmanager.memory.flink.size: 1024m
      taskmanager.numberOfTaskSlots: 1
      parallelism.default: 1
      jobmanager.execution.failover-strategy: region
      taskmanager.memory.network.fraction: 0.1

  statusjob:
    statusjob: |+
      include file("/data/flink/conf/base-config.conf")
      kafka.groupId:  ${job.env}"-status-job-group"
      kafka.input.topic: {{ env }}.hcx.request.status.search
      task.consumer.parallelism :  1
      task.parallelism :  1
      task.downstream.operators.parallelism: 1
      kafka.producer.max-request-size: 1572864
      kafka.producer.batch.size: 98304
      kafka.producer.linger.ms: 10
      #Redis
      redisdb.assetstore.id = 1

    flink-conf: |+
      jobmanager.memory.flink.size: 1024m
      taskmanager.memory.flink.size: 1024m
      taskmanager.numberOfTaskSlots: 1
      parallelism.default: 1
      jobmanager.execution.failover-strategy: region
      taskmanager.memory.network.fraction: 0.1

  searchjob:
    searchjob: |+
      include file("/data/flink/conf/base-config.conf")
      kafka.groupId:  ${job.env}"-search-request-group"
      kafka.input.topic: {{ env }}.hcx.request.search
      task.consumer.parallelism :  1
      task.parallelism :  1
      task.downstream.operators.parallelism: 1
      #Postgres
      postgres.search: composite_search
      kafka.producer.max-request-size: 1572864
      kafka.producer.batch.size: 98304
      kafka.producer.linger.ms: 10
      #Search
      search.time.period = 24
      search.time.maxperiod = 720
      search.entity.types = ["predetermination", "preauth", "claim"]
      search.expiry.time = 86400000
      registry.hcx.code = "hcx-registry-code"

    flink-conf: |+
      jobmanager.memory.flink.size: 1024m
      taskmanager.memory.flink.size: 1024m
      taskmanager.numberOfTaskSlots: 1
      parallelism.default: 1
      jobmanager.execution.failover-strategy: region
      taskmanager.memory.network.fraction: 0.1

  searchresponsejob:
    searchresponsejob: |+
      include file("/data/flink/conf/base-config.conf")
      kafka.groupId:  ${job.env}"-search-response-group"
      kafka.input.topic: {{ env }}.hcx.response.search
      task.consumer.parallelism :  1
      task.parallelism :  1
      task.downstream.operators.parallelism: 1
      #Postgres
      postgres.search: composite_search
      kafka.producer.max-request-size: 1572864
      kafka.producer.batch.size: 98304
      kafka.producer.linger.ms: 10
      #Search
      search.time.period = 24
      search.time.maxperiod = 720
      search.entity.types = ["predetermination", "preauth", "claim"]
      search.expiry.time = 86400000
      registry.hcx.code = "hcx-registry-code"

    flink-conf: |+
      jobmanager.memory.flink.size: 1024m
      taskmanager.memory.flink.size: 1024m
      taskmanager.numberOfTaskSlots: 1
      parallelism.default: 1
      jobmanager.execution.failover-strategy: region
      taskmanager.memory.network.fraction: 0.1

  communicationjob:
    communicationjob: |+
      include file("/data/flink/conf/base-config.conf")
      kafka.groupId:  ${job.env}"-communication-group"
      kafka.input.topic: {{ env }}.hcx.request.communication
      kafka.audit.topic: {{ env }}.hcx.audit
      task.consumer.parallelism :  1
      task.parallelism :  1
      task.downstream.operators.parallelism: 1
      kafka.producer.max-request-size: 1572864
      kafka.producer.batch.size: 98304
      kafka.producer.linger.ms: 10

    flink-conf: |+
      jobmanager.memory.flink.size: 1024m
      taskmanager.memory.flink.size: 1024m
      taskmanager.numberOfTaskSlots: 1
      parallelism.default: 1
      jobmanager.execution.failover-strategy: region
      taskmanager.memory.network.fraction: 0.1

  predeterminationjob:
    predeterminationjob: |+
      include file("/data/flink/conf/base-config.conf")
      kafka.groupId:  ${job.env}"-predetermination-group"
      kafka.input.topic: {{ env }}.hcx.request.predetermination
      kafka.audit.topic: {{ env }}.hcx.audit
      task.consumer.parallelism :  1
      task.parallelism :  1
      task.downstream.operators.parallelism: 1
      kafka.producer.max-request-size: 1572864
      kafka.producer.batch.size: 98304
      kafka.producer.linger.ms: 10

    flink-conf: |+
      jobmanager.memory.flink.size: 1024m
      taskmanager.memory.flink.size: 1024m
      taskmanager.numberOfTaskSlots: 1
      parallelism.default: 1
      jobmanager.execution.failover-strategy: region
      taskmanager.memory.network.fraction: 0.1

  retryjob:
    retryjob: |+
      include file("/data/flink/conf/base-config.conf")
      kafka.groupId:  ${job.env}"-retry-group"
      kafka.input.topic: {{ env }}.hcx.request.retry
      task.consumer.parallelism :  1
      task.parallelism :  1
      task.downstream.operators.parallelism: 1
      kafka.producer.max-request-size: 1572864
      kafka.producer.batch.size: 98304
      kafka.producer.linger.ms: 10

    flink-conf: |+
      jobmanager.memory.flink.size: 1024m
      taskmanager.memory.flink.size: 1024m
      taskmanager.numberOfTaskSlots: 1
      parallelism.default: 1
      jobmanager.execution.failover-strategy: region
      taskmanager.memory.network.fraction: 0.1

  notificationjob:
    notificationjob: |+
      include file("/data/flink/conf/base-config.conf")
      kafka.groupId:  ${job.env}"-notification-group"
      kafka.input.topic: {{ env }}.hcx.request.notification
      kafka.subscription.input.topic = {{ env }}.hcx.request.subscription
      task.consumer.parallelism :  1
      task.parallelism :  1
      task.downstream.operators.parallelism: 1
      task.downstream.operators.dispatcher.parallelism: 1
      kafka.producer.max-request-size: 1572864
      kafka.producer.batch.size: 98304
      kafka.producer.linger.ms: 10
      postgres.subscription.table: "subscription"

    flink-conf: |+
      jobmanager.memory.flink.size: 1024m
      taskmanager.memory.flink.size: 1024m
      taskmanager.numberOfTaskSlots: 1
      parallelism.default: 1
      jobmanager.execution.failover-strategy: region
      taskmanager.memory.network.fraction: 0.1

  notificationtriggerjob:
    notificationtriggerjob: |+
      include file("/data/flink/conf/base-config.conf")
      kafka.groupId:  ${job.env}"-notification-trigger-group"
      kafka.input.topic: {{ env }}.hcx.audit
      kafka.output.topic: {{ env }}.hcx.request.notification
      task.consumer.parallelism :  1
      task.parallelism :  1
      task.downstream.operators.parallelism: 1
      task.downstream.operators.dispatcher.parallelism: 1
      kafka.producer.max-request-size: 1572864
      kafka.producer.batch.size: 98304
      kafka.producer.linger.ms: 10
      postgres.subscription.table: "subscription"
      notification.network.enabled: true
      notification.triggers.disabled: []
      notification.workflow.topicCodeAndAPIActionMap: {"/claim/submit":"notif-claim-initiation", "/claim/on_submit":"notif-claim-closure", "/paymentnotice/request":"notif-payment-initiation", "/paymentnotice/on_request": "notif-payment-closure"}

    flink-conf: |+
      jobmanager.memory.flink.size: 1024m
      taskmanager.memory.flink.size: 1024m
      taskmanager.numberOfTaskSlots: 1
      parallelism.default: 1
      jobmanager.execution.failover-strategy: region
      taskmanager.memory.network.fraction: 0.1